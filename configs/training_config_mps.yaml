# MPS-optimized training configuration

data:
  # Larger dataset since MPS is faster
  train_samples: 20000 # Increased from 10000
  val_samples: 2000 # Increased from 1000
  max_input_length: 384 # Match model config
  max_target_length: 96 # Increased from 64

  # Data loading optimizations for MPS
  num_workers: 4 # More workers for MPS
  pin_memory: false # Not needed for MPS
  prefetch_factor: 4 # Higher prefetch for unified memory

training:
  # MPS-optimized batch sizes
  batch_size: 6 # Increased from 4 (MPS can handle more)
  gradient_accumulation_steps: 2 # Reduced since batch size is larger
  effective_batch_size: 12 # 6 * 2 = 12

  # Learning parameters
  learning_rate: 0.0002
  num_epochs: 8 # More epochs with faster training
  warmup_steps: 1000 # Increased warmup
  weight_decay: 0.01
  gradient_clip_norm: 1.0

  # Checkpointing
  save_every: 1000
  eval_every: 500
  max_checkpoints: 5

optimizer:
  name: "AdamW"
  betas: [0.9, 0.999]
  eps: 1e-8
  amsgrad: false

scheduler:
  name: "linear_with_warmup"
  warmup_ratio: 0.1
  min_lr: 1e-6

# MPS-specific optimizations
mps_optimizations:
  enable_fallback: true # Fallback to CPU for unsupported ops
  memory_fraction: 0.8 # Use 80% of unified memory
  mixed_precision: false # Not supported on MPS yet
  compile_model: false # torch.compile not ready for MPS

# Memory management for unified memory
memory:
  max_memory_gb: 12 # Adjust based on your Mac's RAM
  empty_cache_frequency: 50 # More frequent cache clearing

logging:
  wandb_project: "transformer-summarizer-mps"
  wandb_tags: ["mps", "apple-silicon", "medium-model"]
  log_every: 25
  log_memory_usage: true
  log_device_utilization: true

# Development phases for MPS
phases:
  proof_of_concept:
    train_samples: 2000
    val_samples: 200
    num_epochs: 2
    batch_size: 4

  small_scale:
    train_samples: 8000
    val_samples: 800
    num_epochs: 4
    batch_size: 6

  production:
    train_samples: 25000
    val_samples: 2500
    num_epochs: 10
    batch_size: 8
